# ğŸ¡ Housing Price Prediction â€“ Regression Models Comparison

## ğŸ“Œ Project Overview
This project predicts **housing prices** using multiple **regression and machine learning algorithms**.  
It serves as a benchmark study to compare traditional and advanced ML models, identify the best-performing techniques, and build a foundation for scalable deployment.

---

## ğŸ¯ Objectives
- Understand housing market drivers and their impact on prices.  
- Apply a variety of regression algorithms.  
- Compare model accuracy using industry-standard metrics.  
- Prepare models for potential deployment in production environments.  

---

## ğŸ—‚ï¸ Dataset
- **Source**: USA Housing dataset  
- **Target**: `Price`  
- **Features**:  
  - Avg. Area Income  
  - Avg. Area House Age  
  - Avg. Area Number of Rooms  
  - Avg. Area Number of Bedrooms  
  - Area Population  

---

## ğŸ§  Models Implemented
- **Linear Models**: Linear Regression, Ridge, Lasso, ElasticNet  
- **Polynomial & Regularization**: Polynomial Regression, HuberRegressor  
- **Ensemble & Boosting**: Random Forest, XGBoost, LightGBM  
- **Others**: SVR, KNN, SGDRegressor, MLPRegressor (Neural Net)  

---

## ğŸ“Š Evaluation Metrics
- **Mean Absolute Error (MAE)**  
- **Mean Squared Error (MSE)**  
- **RÂ² Score**  

ğŸ“Œ **Key Results**:  
- **XGBoost, LightGBM, Random Forest** â†’ RÂ² ~0.96â€“0.97 (Top performers)  
- **MLPRegressor** â†’ RÂ² ~0.93 (Strong neural network baseline)  
- **Polynomial Regression & SGDRegressor** â†’ Underperformed due to overfitting/instability  

---

## ğŸš€ How to Run
```bash
git clone https://github.com/sunandini123/Housing_REGRESSOR.git
cd Housing_REGRESSOR
pip install -r requirements.txt
python model.py


# Housing Price Prediction ğŸ“ŠğŸ 

This project uses multiple regression models to predict housing prices. Evaluation metrics include MAE, MSE, and RÂ² across models like Linear Regression, Random Forest, XGBoost, and more.

## Files Included
- `model.py`: All model training and evaluation code
- `model_evaluation_results.csv`: Comparison of model performance
 
## How to Run
1. Clone the repo
2. Install required libraries using: `pip install -r requirements.txt`
3. Run `model.py` to train models and generate results

## Dataset
Sample housing dataset used.


---

## ğŸ“Š Dataset Overview

- **Features**: Avg. Area Income, Avg. House Age, Avg. Number of Rooms, Population, etc.
- **Target**: `Price` â€“ housing price in USD

---

## âš™ï¸ Models Trained

| Model               | Type                |
|---------------------|---------------------|
| LinearRegression     | Linear              |
| Ridge, Lasso, ElasticNet | Regularized Linear |
| HuberRegressor       | Robust              |
| Polynomial (Degree=4)| Non-linear          |
| SGDRegressor         | Online Learning     |
| MLPRegressor         | Neural Network      |
| RandomForest         | Ensemble (Trees)    |
| SVR                  | Support Vector      |
| LGBM, XGBoost        | Gradient Boosting   |
| KNN                  | Distance-Based      |

---

## ğŸ“ˆ Evaluation Metrics

Each model is evaluated on:

| Metric | Meaning |
|--------|---------|
| **MAE** (Mean Absolute Error) | Avg. error in predictions. Lower is better. |
| **MSE** (Mean Squared Error) | Penalizes larger errors more than MAE. |
| **RÂ² Score** | Measures goodness-of-fit. <br>**1 = perfect**, **0 = baseline**, **< 0 = worse than baseline**. |

---

## ğŸ“‰ Results: Interpreting Metrics

| Model              | MAE         | MSE         | RÂ² Score     | Interpretation |
|-------------------|-------------|-------------|--------------|----------------|
| LinearRegression   | 8.10e+04    | 1.01e+10    | 0.919        | âœ… Very good baseline |
| RidgeRegression    | ~8.1e+04    | ~1.01e+10   | ~0.919       | âœ… Similar to Linear |
| PolynomialRegression | Extremely high MAE & MSE | RÂ² < 0 | âŒ Severely overfit |
| SGDRegressor       | MAE > 1e+18 | MSE > 1e+36 | RÂ² = -1e+25  | âŒ Completely diverged |
| MLPRegressor       | Good MAE/RÂ² | Around 0.93 | âœ… Performs well |
| RandomForest       | ~4.0e+04    | Lower MSE   | 0.965+       | âœ… Best performing |
| XGBoost / LGBM     | Similar to RF| ~0.96â€“0.97 | âœ… Robust models |
| SVR / KNN          | Higher error | RÂ² ~0.7     | âš ï¸ Weak fit |
| Huber              | Similar to Linear | ~0.91 | âœ… Robust baseline alternative |

> ğŸ” **RÂ² < 0** (e.g. in Polynomial, SGD) means the model performed worse than a simple average prediction. Likely due to overfitting, bad convergence, or poor hyperparameters.

---

## ğŸ§  Key Insights

- **RandomForest, XGBoost, and LGBM** had the best overall performance with high RÂ² and low errors.
- **Polynomial Regression** and **SGDRegressor** overfit or diverged. Not suitable without tuning.
- **Regularized models (Ridge, Lasso)** perform similarly to baseline linear regression.
- **MLP (Neural Net)** is decent, but slower to train.

---
