# Housing Price Prediction ğŸ“ŠğŸ 

This project uses multiple regression models to predict housing prices. Evaluation metrics include MAE, MSE, and RÂ² across models like Linear Regression, Random Forest, XGBoost, and more.

## Files Included
- `model.py`: All model training and evaluation code
- `model_evaluation_results.csv`: Comparison of model performance

## How to Run
1. Clone the repo
2. Install required libraries using: `pip install -r requirements.txt`
3. Run `model.py` to train models and generate results

## Dataset
Sample housing dataset used.


---

## ğŸ“Š Dataset Overview

- **Features**: Avg. Area Income, Avg. House Age, Avg. Number of Rooms, Population, etc.
- **Target**: `Price` â€“ housing price in USD

---

## âš™ï¸ Models Trained

| Model               | Type                |
|---------------------|---------------------|
| LinearRegression     | Linear              |
| Ridge, Lasso, ElasticNet | Regularized Linear |
| HuberRegressor       | Robust              |
| Polynomial (Degree=4)| Non-linear          |
| SGDRegressor         | Online Learning     |
| MLPRegressor         | Neural Network      |
| RandomForest         | Ensemble (Trees)    |
| SVR                  | Support Vector      |
| LGBM, XGBoost        | Gradient Boosting   |
| KNN                  | Distance-Based      |

---

## ğŸ“ˆ Evaluation Metrics

Each model is evaluated on:

| Metric | Meaning |
|--------|---------|
| **MAE** (Mean Absolute Error) | Avg. error in predictions. Lower is better. |
| **MSE** (Mean Squared Error) | Penalizes larger errors more than MAE. |
| **RÂ² Score** | Measures goodness-of-fit. <br>**1 = perfect**, **0 = baseline**, **< 0 = worse than baseline**. |

---

## ğŸ“‰ Results: Interpreting Metrics

| Model              | MAE         | MSE         | RÂ² Score     | Interpretation |
|-------------------|-------------|-------------|--------------|----------------|
| LinearRegression   | 8.10e+04    | 1.01e+10    | 0.919        | âœ… Very good baseline |
| RidgeRegression    | ~8.1e+04    | ~1.01e+10   | ~0.919       | âœ… Similar to Linear |
| PolynomialRegression | Extremely high MAE & MSE | RÂ² < 0 | âŒ Severely overfit |
| SGDRegressor       | MAE > 1e+18 | MSE > 1e+36 | RÂ² = -1e+25  | âŒ Completely diverged |
| MLPRegressor       | Good MAE/RÂ² | Around 0.93 | âœ… Performs well |
| RandomForest       | ~4.0e+04    | Lower MSE   | 0.965+       | âœ… Best performing |
| XGBoost / LGBM     | Similar to RF| ~0.96â€“0.97 | âœ… Robust models |
| SVR / KNN          | Higher error | RÂ² ~0.7     | âš ï¸ Weak fit |
| Huber              | Similar to Linear | ~0.91 | âœ… Robust baseline alternative |

> ğŸ” **RÂ² < 0** (e.g. in Polynomial, SGD) means the model performed worse than a simple average prediction. Likely due to overfitting, bad convergence, or poor hyperparameters.

---

## ğŸ§  Key Insights

- **RandomForest, XGBoost, and LGBM** had the best overall performance with high RÂ² and low errors.
- **Polynomial Regression** and **SGDRegressor** overfit or diverged. Not suitable without tuning.
- **Regularized models (Ridge, Lasso)** perform similarly to baseline linear regression.
- **MLP (Neural Net)** is decent, but slower to train.

---
